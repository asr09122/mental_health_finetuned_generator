{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "dataset_path = \"hinglish_journal_dataset2_10k_shuffled.jsonl\"  # adjust path if needed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f0583de65342a08e2151a148e9974c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,   # âœ… use bf16 on L40S\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,175,040 || all params: 3,221,924,864 || trainable%: 0.2848\n"
     ]
    }
   ],
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531aafa899684cf9ac4fef289bfb2845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"json\", data_files=dataset_path)[\"train\"]\n",
    "\n",
    "# Format into Alpaca-style instruction data\n",
    "def format_ex(ex):\n",
    "    return {\n",
    "        \"text\": f\"### Journal:\\n{ex['journal']}\\n\\n### Reflection:\\n{ex['reflection']}{tokenizer.eos_token}\"\n",
    "    }\n",
    "\n",
    "ds = ds.map(format_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558aa6de7b5744448b797bc312e4fe52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "    return enc\n",
    "\n",
    "tok_ds = ds.map(tokenize, batched=True, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"/teamspace/studios/this_studio/lora_adapter\",\n",
    "    num_train_epochs=2,                  # can increase to 3 later\n",
    "    per_device_train_batch_size=4,       # âœ… fits on 48 GB GPU\n",
    "    gradient_accumulation_steps=8,       # effective batch size = 32\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    optim=\"paged_adamw_32bit\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='404' max='404' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [404/404 26:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.357200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.237200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.092300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.075000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tok_ds,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA adapter\n",
    "adapter_path = \"/teamspace/studios/this_studio/lora_adapter\"\n",
    "model.save_pretrained(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4acff97b0746a491d1fdd997f8bafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded!\n",
      "\n",
      "ðŸ”¹ Testing with mixed sentiment journal entries:\n",
      "\n",
      "--- Example 1 ---\n",
      "Journal: Aaj ka din accha tha, lekin thoda guilt feel hua kyunki gym skip kar diya.\n",
      "**Motivation**\n",
      "It's amazing you took time to acknowledge your feelings; acknowledging our emotions is really brave! Keep moving forward, even small steps like going to the gym are worth it for your well-being.\n",
      "\n",
      "**Improvement Tips**\n",
      "Here are some tips that might help you connect with your inner motivation: try writing down three things you enjoyed doing today or did well for yourself. It can also be helpful to set smaller goals so your daily routine doesn't feel overwhelming.\n",
      "\n",
      "**Guided Resources**\n",
      "Did you know there's an app called Fit\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "--- Example 2 ---\n",
      "Journal: Meri best friend se fight ho gayi, ab lag raha hai shayad galti meri thi.\n",
      "**Motivation**\n",
      "Kahi baar chhoti-choti zindagi ke issues le kar sab kuch sambhalna mushkil hota hai. Aapki feelings bilkul valid hain, aur it's okay to express them; remember, you're stronger than these temporary storms.\n",
      "\n",
      "**Improvement Tips**\n",
      "Try practicing mindfulness exercises or deep breathing to calm your thoughts while reflecting on those difficult moments. It might also help talking through your emotions with someone else who truly cares about you.\n",
      "\n",
      "**Guided Resources**\n",
      "Check out these helpful resources for coping with social stress: \"A Gentle Introduction to Meditation\" by Dr.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "--- Example 3 ---\n",
      "Journal: Workload itna zyada hai ki anxiety ho rahi hai, bas chhutti chahiye.\n",
      "**Motivation**\n",
      "It's completely okay to feel overwhelmed; you deserve breaks and self-care! Remember, taking care of your mental health is just as important as meeting deadlines.\n",
      "\n",
      "**Improvement Tips**\n",
      "Try incorporating short, regular pauses into your day. Even a few deep breaths can help calm your mind and refocus your energy. You could also consider talking to a supervisor or colleague about your workload concerns; sometimes sharing your struggles can make things more manageable.\n",
      "\n",
      "**Guided Resources**\n",
      "If you need some extra support during stressful moments, check out \"\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "--- Example 4 ---\n",
      "Journal: Aaj ghar pe family ke saath maza aaya, lekin thoda stress bhi tha kal ke exam ka.\n",
      "Motivation:\n",
      "It's wonderful that you got to spend time with your loved ones! Even amidst excitement, it can be tough to deal with expectations from them. Remember, taking care of yourself is just as important as meeting those expectations.\n",
      "\n",
      "Improvement Tips:\n",
      "Try to break down larger tasks into smaller chunks for better focus during exams. Regular exercise or short breaks can also help in reducing anxiety levels. Communicate openly with your family about whatâ€™s really going on; sometimes they donâ€™t realize how overwhelming everything feels.\n",
      "\n",
      "Guided Resources:\n",
      "For managing exam stress, check out \"The Ultimate Guide to Exam Stress\" by Time Magazine (https://time\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "--- Example 5 ---\n",
      "Journal: Kal raat neend nahi aayi, mann bahut heavy tha aur khud pe doubt hua.\n",
      "Motivation\n",
      "Aapka struggle ko samajhna bahut koshish kar raha hoon. It's completely okay to feel overwhelmed sometimes; just remember you're doing your best, and that's what truly matters.\n",
      "\n",
      "Improvement Tips\n",
      "Mujhe aisa laga hai ki exercise se thoda stress release ho sakta hai. Chalo gym jaate hue shuru karein ya yeh walk karo jisse thoda fresh feel karein. Apne thoughts ko write karna bhi ek great way ishaara hota hai ki ye feelings kya hain, apne aap ko sambhalne mein madad karega.\n",
      "\n",
      "Guided Resources\n",
      "Dosto ke saath connect hona bahut important hota hai. Apni dhadkan share karke uss relief mil sake, bas sabhi log hi aapki zindagi kaafi mushkile se guzar rahe hote hain. Aapko chinta mat karo, ye moment bhi guzarta hai!\n",
      "\n",
      "Closing Note\n",
      "You're stronger than this night, and itâ€™s okay to take steps for yourself. Keep shining bright, remember that we're all in this together. Take small moments, like breathing deeply or smiling at a friend, to lift your spirits. You got this!\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interface for fine-tuned model with motivation, mixed sentiments, tips & resources\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import re\n",
    "\n",
    "# ---------------------------\n",
    "# Load model and tokenizer\n",
    "# ---------------------------\n",
    "def load_model():\n",
    "    base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    adapter_path = \"/teamspace/studios/this_studio/lora_adapter\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# ---------------------------\n",
    "# Clean text\n",
    "# ---------------------------\n",
    "def clean_output(text):\n",
    "    # Remove unwanted markers and notes\n",
    "    text = re.sub(r\"(###|Note:).*\", \"\", text, flags=re.DOTALL).strip()\n",
    "    return text\n",
    "\n",
    "# ---------------------------\n",
    "# Generate reflection\n",
    "# ---------------------------\n",
    "def generate_reflection(model, tokenizer, journal_text):\n",
    "    prompt = (\n",
    "        \"You are a compassionate mental health reflection assistant.\\n\"\n",
    "        \"For the given journal entry, generate output in 4 clear sections:\\n\\n\"\n",
    "        \"1. Motivation â†’ empathetic and encouraging (can be positive OR mixed depending on the journal tone).\\n\"\n",
    "        \"2. Improvement Tips â†’ 2â€“4 practical, culturally relevant suggestions (study, career, fitness, relationships, mental health).\\n\"\n",
    "        \"3. Guided Resources â†’ 2â€“3 helpful links (YouTube talks, meditation apps, motivational articles, podcasts).\\n\"\n",
    "        \"4. Closing Note â†’ warm, human-like closing lines that leave hope & reassurance.\\n\\n\"\n",
    "        \"Keep the response realistic, emotionally nuanced, and not repetitive.\\n\\n\"\n",
    "        f\"Journal Entry:\\n{journal_text}\\n\\nResponse:\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=700,   # allow long responses\n",
    "            temperature=0.85,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = generated_text.split(\"Response:\")[-1].strip()\n",
    "    return clean_output(response)\n",
    "\n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading model...\")\n",
    "    model, tokenizer = load_model()\n",
    "    print(\"âœ… Model loaded!\")\n",
    "\n",
    "    # Example journals\n",
    "    example_entries = [\n",
    "        \"Aaj ka din accha tha, lekin thoda guilt feel hua kyunki gym skip kar diya.\",\n",
    "        \"Meri best friend se fight ho gayi, ab lag raha hai shayad galti meri thi.\",\n",
    "        \"Workload itna zyada hai ki anxiety ho rahi hai, bas chhutti chahiye.\",\n",
    "        \"Aaj ghar pe family ke saath maza aaya, lekin thoda stress bhi tha kal ke exam ka.\",\n",
    "        \"Kal raat neend nahi aayi, mann bahut heavy tha aur khud pe doubt hua.\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nðŸ”¹ Testing with mixed sentiment journal entries:\\n\")\n",
    "    for i, entry in enumerate(example_entries, 1):\n",
    "        print(f\"--- Example {i} ---\")\n",
    "        print(f\"Journal: {entry}\")\n",
    "        reflection = generate_reflection(model, tokenizer, entry)\n",
    "        print(f\"{reflection}\\n\")\n",
    "        print(\"=\"*100 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
